{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "from tkinter import filedialog\n",
    "\n",
    "# A function to allow the user to select the folder contianing the data.\n",
    "# Function inputs args: None. \n",
    "# Function output 1: The path of that the folder selected by the user. \n",
    "def folder_selection_dialog():\n",
    "    root = Tk()\n",
    "    root.title('Please select the directory containing the images')\n",
    "    root.filename = filedialog.askdirectory(initialdir=\"/\", title=\"Select A Folder\")\n",
    "    directory = root.filename\n",
    "    root.destroy()\n",
    "\n",
    "    return directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Function to consider existing images, then crop them to a smaller size to make training a bit easier. This function will not be used within the RUNME files.\n",
    "def crop_images():\n",
    "    \n",
    "    # Select the directory with the images to crop. \n",
    "    directory = folder_selection_dialog()\n",
    "    \n",
    "    # Get a list of the images. \n",
    "    image_list = [_ for _ in os.listdir(directory) if 'C01.tif' in _]\n",
    "    \n",
    "    # Make their folder. \n",
    "    if not os.path.exists(os.path.join(directory, 'smaller_images')):\n",
    "        os.makedirs(os.path.join(directory, 'smaller_images'))\n",
    "    \n",
    "    # Iterate through each of the images and get 4 smaller images from each.\n",
    "    x = 1\n",
    "    for i in range(len(image_list)): \n",
    "        \n",
    "        # Load in the image. \n",
    "        image_i = np.array(Image.open(os.path.join(directory, image_list[i])))\n",
    "        l = 1024 # This is an arbitrary value, selected for ease of building this model and developing this skill set. \n",
    "        \n",
    "        # Get the top left cropped image and save it. \n",
    "        image_TL = image_i[0:l,0:l] \n",
    "        im = Image.fromarray(image_TL)\n",
    "        im.save(os.path.join(directory, 'smaller_images', f'image_{str(x)}.tif'))\n",
    "        x += 1\n",
    "        \n",
    "        # Get the top right cropped image and save it.                      \n",
    "        image_TR = image_i[0:l,l:l*2]\n",
    "        im = Image.fromarray(image_TR)\n",
    "        im.save(os.path.join(directory, 'smaller_images', f'image_{str(x)}.tif'))\n",
    "        x += 1\n",
    "         \n",
    "        # Get the bottom left cropped image and save it.                      \n",
    "        image_BL = image_i[l:l*2,0:l]\n",
    "        im = Image.fromarray(image_BL)\n",
    "        im.save(os.path.join(directory, 'smaller_images', f'image_{str(x)}.tif'))\n",
    "        x += 1\n",
    "         \n",
    "        # Get the bottom right cropped image and save it.                      \n",
    "        image_BR = image_i[l:l*2,l:l*2]\n",
    "        im = Image.fromarray(image_BR)\n",
    "        im.save(os.path.join(directory, 'smaller_images', f'image_{str(x)}.tif'))\n",
    "        x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "from tkinter import filedialog\n",
    "\n",
    "# A function to allow the user to select the model they wish to use or retrain. \n",
    "# Function inputs args: None. \n",
    "# Function output 1: The file path of that which was selected by the user. \n",
    "def file_selection_dialog():\n",
    "    root = Tk()\n",
    "    root.title('Please select the machine learning model in question')\n",
    "    root.filename = filedialog.askopenfilename(initialdir=\"/\", title=\"Select A File\", filetypes=[(\"All files\", \"*.*\")])\n",
    "    file_path = root.filename\n",
    "    root.destroy()\n",
    "\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Function to display and save the training loss and validation loss per epoch.\n",
    "# Function input arg 1: training_loss --> Array of size 1 x num_epochs. This array contains the calculated values of loss for training. \n",
    "# Function input arg 2: validation_loss --> Array of size 1 x num_epochs. This array contains the calculated values of loss for validation. \n",
    "# Function input arg 3: save_plot --> True or Flase. When true, saves plot to data directory.  \n",
    "# Function input arg 4: display_plot --> True or Flase. When true, displays the plot. \n",
    "# Function input arg 5: directory --> The directory containing the training dataset. \n",
    "# Function input arg 6: date_time --> The datetime string in the format of 'YMD_HMS'. \n",
    "def loss_graph(training_loss, \n",
    "               validation_loss, \n",
    "               save_plot, \n",
    "               display_plot,\n",
    "               directory, \n",
    "               date_time):\n",
    "    \n",
    "    # Plot the loss per epoch. \n",
    "    y = list(range(0,len(training_loss)))\n",
    "    plt.plot(y, training_loss, label = \"Training loss\")\n",
    "    plt.plot(y, validation_loss, label = \"Validation loss\")\n",
    "    plt.rcParams.update({'font.size': 15})\n",
    "    plt.ylabel('Loss', labelpad=10) # The labelpad argument alters the distance of the axis label from the axis itself. \n",
    "    plt.xlabel('Epoch', labelpad=10)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "\n",
    "    # Save the plot if the user desires it.\n",
    "    if save_plot:\n",
    "        folder_name = f'training data_{date_time}'\n",
    "        if not os.path.exists(os.path.join(directory, folder_name)):\n",
    "            os.makedirs(os.path.join(directory, folder_name))\n",
    "        file_path = os.path.join(directory, folder_name, f'loss_{date_time}.png')\n",
    "        plt.savefig(file_path, dpi=200, bbox_inches='tight')\n",
    "    \n",
    "    # Display the plot if the user desires it. \n",
    "    if (display_plot == False):\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Function to display and save the training accuracy and validation accuracy per epoch.\n",
    "# Function input arg 1: training_accuracy --> Array of size 1 x num_epochs. This array contains the calculated values of training accuracy. \n",
    "# Function input arg 2: validation_accuracy --> Array of size 1 x num_epochs. This array contains the calculated values of validation accuracy. \n",
    "# Function input arg 3: save_plot --> True or Flase. When true, saves plot to data directory.  \n",
    "# Function input arg 4: display_plot --> True or Flase. When true, displays the plot. \n",
    "# Function input arg 5: directory --> The directory containing the training dataset. \n",
    "# Function input arg 6: date_time --> The datetime string in the format of 'YMD_HMS'. \n",
    "def accuracy_graph(training_accuracy, \n",
    "                   validation_accuracy, \n",
    "                   save_plot, \n",
    "                   display_plot,\n",
    "                   directory, \n",
    "                   date_time):\n",
    "    \n",
    "    # Plot the BCE calculated loss per epoch. \n",
    "    y = list(range(0,len(training_accuracy)))\n",
    "    plt.plot(y, training_accuracy, label=\"Training accuracy\")\n",
    "    plt.plot(y, validation_accuracy, label=\"Validation accuracy\")\n",
    "    plt.rcParams.update({'font.size': 15})\n",
    "    plt.ylabel('Accuracy', labelpad=10) # The leftpad argument alters the distance of the axis label from the axis itself. \n",
    "    plt.xlabel('Epoch', labelpad=10)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "\n",
    "    # Save the plot if the user desires it.\n",
    "    if save_plot:\n",
    "        folder_name = f'training data_{date_time}'\n",
    "        if not os.path.exists(os.path.join(directory, folder_name)):\n",
    "            os.makedirs(os.path.join(directory, folder_name))\n",
    "        file_path = os.path.join(directory, folder_name, f'accuracy_{date_time}.png')\n",
    "        plt.savefig(file_path, dpi=200, bbox_inches='tight')\n",
    "    \n",
    "    # Display the plot if the user desires it. \n",
    "    if (display_plot == False):\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "import os\n",
    "from math import floor \n",
    "\n",
    "# A function which will append images within a directory into a numpy array.\n",
    "# Function input 1: directory [string] --> The directory containing the images.\n",
    "# Function input 2: file_type [string] --> The file type of the training images e.g. '.tif'.\n",
    "# Function input 3: indices [list] --> The list of image indices to indicate which images and which transformations to use. \n",
    "# Function output 1: image stack [numpy array] --> The 3D stack of appended images. \n",
    "def append_images(directory,\n",
    "                  file_type,\n",
    "                  indices):\n",
    "\n",
    "    # Create an empty list. \n",
    "    image_stack = []\n",
    "    \n",
    "    # Iterate through the images of our list and append them to our stack. \n",
    "    for i in range(len(indices)):\n",
    "        \n",
    "        # Determine which unmodified image we need to use. \n",
    "        image_number = floor(indices[i]/8)+1\n",
    "        image_name = f\"image_{str(image_number)}{file_type}\"\n",
    "        \n",
    "        file_path = os.path.join(directory, image_name)\n",
    "        img = cv2.imread(file_path, -1)\n",
    "        \n",
    "        # Scale the image between -1 and 1. \n",
    "        img = np.interp(img, (img.min(), img.max()), (-1, +1))\n",
    "        \n",
    "        # Determine which transformation to apply. \n",
    "        transformation_1 = (indices[i]) % 8\n",
    "        transformation_2 = (indices[i]) % 4\n",
    "        \n",
    "        # Rotation\n",
    "        if 0 <= transformation_1 <= 3:\n",
    "            img = np.rot90(img, 1)\n",
    "        \n",
    "        # Horizontal flipping.\n",
    "        if 1 <= transformation_2 <= 2:\n",
    "            img = np.flip(img, axis=0) \n",
    "        \n",
    "        # Vertical flipping.\n",
    "        if 2 <= transformation_2 <= 3:\n",
    "            img = np.flip(img, axis=1) \n",
    "\n",
    "        # Add an extra axis (for processing later) and append our image to the stack.\n",
    "        img = np.stack((img,)*1, axis=-1)\n",
    "        image_stack.append(img)\n",
    "\n",
    "    # Convert the stack to a numpy array. \n",
    "    image_stack = np.asarray(image_stack)\n",
    "\n",
    "    return image_stack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LeakyReLU, Reshape, Conv2DTranspose, Conv2D, Dropout, BatchNormalization\n",
    "\n",
    "# Funtion to create our generator. \n",
    "# Function input 1: latent_size [int] --> Size of the 1D latent space vector. \n",
    "# Function input 2: img_height [int] --> Image height in pixels. \n",
    "# Function input 3: img_width [int] --> Image width in pixels. \n",
    "# Function output 1: model --> The untrained model. \n",
    "def create_generator(latent_size,\n",
    "                     img_height,\n",
    "                     img_width):\n",
    "    \n",
    "    # Create the backbone for our sequential model.\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First, we need to create smaller 4x4 images which can be upscaled.\n",
    "    # To add redundancy to our model, we'll make 'i' such images. \n",
    "    n_nodes = 4 * 4 * 64\n",
    "    model.add(Dense(n_nodes, input_dim=latent_size))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    # We then need to reshape this Dense layer from a single dimension to 3 dimensions. \n",
    "    model.add(Reshape((4,4,64)))\n",
    "    \n",
    "    # Updample to 8x8: use 64 differnt convolutions, of size 4x4. \n",
    "    model.add(Conv2DTranspose(64, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    \n",
    "    # Upsample to 16x16: use 64 differnt convolutions, of size 4x4. \n",
    "    model.add(Conv2DTranspose(64, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    \n",
    "    # Upsample to 32x32: use 64 differnt convolutions, of size 4x4. \n",
    "    model.add(Conv2DTranspose(64, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    \n",
    "    # Upsample to 64x64: use 64 differnt convolutions, of size 4x4. \n",
    "    model.add(Conv2DTranspose(64, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    \n",
    "    # Upsample to 128x128: use 64 differnt convolutions, of size 4x4. \n",
    "    model.add(Conv2DTranspose(64, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    \n",
    "    # Upsample to 256x256: use 64 differnt convolutions, of size 4x4. \n",
    "    model.add(Conv2DTranspose(64, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    \n",
    "    # Upsample to 512x512: use 64 differnt convolutions, of size 4x4. \n",
    "    model.add(Conv2DTranspose(64, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    \n",
    "    # Upsample to 1024x1024: use 64 differnt convolutions, of size 4x4. \n",
    "    model.add(Conv2DTranspose(64, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    \n",
    "    # Create our output layer.\n",
    "    # Use a tanh activation to scale our pixel values between -1 and +1. \n",
    "    model.add(Conv2D(1, (3,3), activation='tanh', padding='same'))\n",
    "    \n",
    "    # Return the model as the function output. \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "# Function to create the latent space (the generator's input). \n",
    "# Function input 1: latent_dimension [int] --> The size of the latent dimension for a single sample. In my case, it's 100.\n",
    "# Function input 2: number_of_samples [int] --> The number of samples for which we need to create a latent space. \n",
    "# Function output: latent_space [array] --> The latent spaces for all the samples. \n",
    "def create_latent_space(latent_dimension,\n",
    "                        number_of_samples): \n",
    "    \n",
    "    # First, we must create random numbers adhering to a normal (gaussian) distribution.\n",
    "    latent_space = np.random.randn(latent_dimension * number_of_samples)\n",
    "    \n",
    "    # Reshape the vector or numbers to a 2D array, such that it can be used to represent numerous samples.\n",
    "    latent_space = latent_space.reshape(number_of_samples, latent_dimension)\n",
    "    \n",
    "    return latent_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "# Function to create fake images using our generator. \n",
    "# Function input 1: generator_model --> The generator model.\n",
    "# Function input 2: latent_dimension [int] --> The size of the latent dimension for a single sample. In my case, it's 100.\n",
    "# Function input 3: number_of_samples [int] --> The number of samples for which we need to create a latent space. \n",
    "def make_fake_images(generator_model,\n",
    "                     latent_dimension,\n",
    "                     number_of_samples):\n",
    "    \n",
    "    # Create the latent space(s).\n",
    "    latent_space = create_latent_space(latent_dimension, number_of_samples)\n",
    "    \n",
    "    # Use our model to create fake images. \n",
    "    fake_images = generator_model.predict(latent_space)\n",
    "    \n",
    "    # Create the corresponding 'fake' class labels (0).\n",
    "    fake_image_labels = np.zeros((number_of_samples, 1))\n",
    "    \n",
    "    return fake_images, fake_image_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Function to select and return a subset of the real images. \n",
    "# Function input 1: num_requested_images [int] --> The number of images you need from the total array. \n",
    "# Function input 2: unused images_indices [list] --> The unused image indexes.\n",
    "# Function output 1: unused_images_indices [list] --> List of image indices which have not yet been used.\n",
    "# Function output 2: real_images_subset [array] --> Array of the real images, organised as [number_of_images, heigth, width, channels].\n",
    "# Function output 3: real_images_labels [array] --> Array of 1s, of the same length as the number of images.\n",
    "def return_real_images(num_requested_images,\n",
    "                       unused_images_indices):\n",
    "    \n",
    "    # Get the minimum and maximum.\n",
    "    number_of_idx = len(unused_images_indices)\n",
    "    \n",
    "    # If we have sufficient images to take a selection from. \n",
    "    if num_requested_images < len(unused_images_indices):\n",
    "        \n",
    "        # Get the indices to select the indices.\n",
    "        indices = random.sample(range(number_of_idx), num_requested_images)\n",
    "        \n",
    "        # Get the corresponding images. \n",
    "        real_images_subset = append_images(directory,\n",
    "                                           file_type,\n",
    "                                           indices)\n",
    "        \n",
    "        # Remove these indices from the unused_images_indices. \n",
    "        unused_images_indices = [_ for _ in unused_images_indices if _ not in indices]\n",
    "\n",
    "        # Get the real image labels.\n",
    "        real_images_labels = np.ones(real_images_subset.shape[0]) \n",
    "        \n",
    "    # If we have an insifficient number of images, take whatever is left. \n",
    "    else:\n",
    "    \n",
    "        # Get the rest of the real images. \n",
    "        real_images_subset = append_images(directory,\n",
    "                                           file_type,\n",
    "                                           unused_images_indices)\n",
    "\n",
    "        # Clear out unused_images_indices.\n",
    "        unused_images_indices = [] \n",
    "        \n",
    "        # Get the real image labels.\n",
    "        real_images_labels = np.ones(real_images_subset.shape[0]) \n",
    "        \n",
    "    return unused_images_indices, real_images_subset, real_images_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dropout, LeakyReLU, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Function to create the discriminator. \n",
    "# Function input arg 1: img_height [int] --> The pixel height of the image.\n",
    "# Function input arg 2: img_width [int] --> The pixels width of the image. \n",
    "# Function input arg 3: img_channels [int] --> The number of channels to the image. \n",
    "def create_discriminator(img_height,\n",
    "                         img_width,\n",
    "                         img_channels): \n",
    "    \n",
    "    # Define the input shape of our images. \n",
    "    image_shape = (img_height, img_width, img_channels)\n",
    "    \n",
    "    # Create an instance of a sequential model. \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Create a normal convolutional layer. \n",
    "    model.add(Conv2D(64, (3,3), padding='same', input_shape=image_shape))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    # Downsample our input to 512 x 512. \n",
    "    model.add(Conv2D(32, (3,3), strides=(2,2), padding='same', input_shape=image_shape))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    # Downsample our input to 256 x 256. \n",
    "    model.add(Conv2D(32, (3,3), strides=(2,2), padding='same', input_shape=image_shape))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    # Downsample our input to 128 x 128. \n",
    "    model.add(Conv2D(32, (3,3), strides=(2,2), padding='same', input_shape=image_shape))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    # Downsample our input to 64 x 64. \n",
    "    model.add(Conv2D(32, (3,3), strides=(2,2), padding='same', input_shape=image_shape))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "    # Downsample our input to 32 x 32. \n",
    "    model.add(Conv2D(64, (3,3), strides=(2,2), padding='same', input_shape=image_shape))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    # Downsample our input to 16 x 16. \n",
    "    model.add(Conv2D(128, (3,3), strides=(2,2), padding='same', input_shape=image_shape))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    # Downsample our input to 8 x 8. \n",
    "    model.add(Conv2D(256, (3,3), strides=(2,2), padding='same', input_shape=image_shape))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    # Downsample our input to 4 x 4. \n",
    "    model.add(Conv2D(256, (3,3), strides=(2,2), padding='same', input_shape=image_shape))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    # Create the classifier. \n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile the model. \n",
    "    optim = Adam(learning_rate=0.002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Function to combine the generator and the discriminator, such that the generator can be trained. \n",
    "# Function input 1: [model instance] --> the \n",
    "def create_GAN(generator_model, \n",
    "               discriminator_model): \n",
    "    \n",
    "    # First, prevent the discriminator from training.\n",
    "    discriminator_model.trainable = False \n",
    "    \n",
    "    # Combine the two models with a sequential model. \n",
    "    model = Sequential()\n",
    "    model.add(generator_model)\n",
    "    model.add(discriminator_model)\n",
    "    \n",
    "    # Compile the model.\n",
    "    optim = Adam(learning_rate=0.002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optim)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil \n",
    "import pandas as pd\n",
    "\n",
    "# Function to train the generator and the discriminator.\n",
    "# Function input arg 1: generator_model [model] --> The generator. \n",
    "# Function input arg 2: discriminator_model [model] --> The discriminator. \n",
    "# Function input arg 3: latent_dimension [int] --> The size of the latent space. \n",
    "# Function input arg 4: num_epochs [int] --> The number of epochs to train for. \n",
    "# Function input arg 5: batch_size [int, preferbly even] --> The total number of images to process per batch. \n",
    "# Function input arg 6: directory [string] --> The directory containing the training images. \n",
    "# Fucntion input arg 7: file_type [string] --> The file type of the training images. \n",
    "def train_GAN(generator_model, \n",
    "              discriminator_model,\n",
    "              latent_dimension,\n",
    "              num_epochs,\n",
    "              batch_size, \n",
    "              directory, \n",
    "              file_type):\n",
    "    \n",
    "    # Calculate the number of batches per epoch.\n",
    "    num_batches = ceil(len([_ for _ in os.listdir(directory) if file_type in _]) / batch_size)\n",
    "    \n",
    "    # Create an empty pandas dataframe to store the training information.\n",
    "    training_data = pd.DataFrame(columns=['Discriminator loss real', ' Discriminator loss fake', 'Discriminator accuracy real', 'Discriminator accuracy fake', 'Generator loss'])\n",
    "    \n",
    "    # Loop through each epoch and train the model. \n",
    "    for i in (x := trange(num_epochs)):\n",
    "\n",
    "        # Set the description for the trange progress bar. \n",
    "        x.set_description(f\"Training GAN model. Epoch number:{str(i)}\")\n",
    "            \n",
    "        # Re-create the list of indexes for the images (which haven't been used for training) at the start of each epoch (it changes with each batch). \n",
    "        unused_images_indices = []\n",
    "        img_in_dir = len([_ for _ in os.listdir(directory) if file_type in _])\n",
    "        for i in range(img_in_dir*8):\n",
    "            unused_images_indices.append(i)\n",
    "\n",
    "        # Create empty lists for storing training data. \n",
    "        batch_discriminator_loss_real = []\n",
    "        batch_discriminator_loss_fake = []\n",
    "        batch_discriminator_accuracy_real = []\n",
    "        batch_discriminator_accuracy_fake = [] \n",
    "        batch_generator_loss = []\n",
    "        \n",
    "        # Loop though each batch and train the model.\n",
    "        for t in range(num_batches): \n",
    "\n",
    "            ###########################\n",
    "            # TRAIN THE DISCRIMINATOR.\n",
    "            ###########################\n",
    "            \n",
    "            # Select a half-batch of real data and update the discriminator. \n",
    "            number_of_samples = int(batch_size/2)\n",
    "            \n",
    "            unused_images_indices, real_img_subset, real_img_labels = return_real_images(number_of_samples,\n",
    "                                                                                         unused_images_indices)\n",
    "            \n",
    "            d_loss_real, d_accuracy_real = discriminator_model.train_on_batch(real_img_subset, \n",
    "                                                             real_img_labels)\n",
    "            \n",
    "            # Select a half-batch of fake data and update the discriminator. \n",
    "            if real_img_subset.shape[0] < number_of_samples:\n",
    "                number_of_samples = real_images_subset.shape[0] # This checks to see whether we have enough real images to 'fill' the bacth. If not, then we create a smaller number of fake images, equal to the number of real images. We don't want to imbalance the model\n",
    "            \n",
    "            fake_images, fake_image_labels = make_fake_images(generator_model,\n",
    "                                                              latent_dimension,\n",
    "                                                              number_of_samples)   \n",
    "            \n",
    "            d_loss_fake, d_accuracy_fake = discriminator_model.train_on_batch(fake_images, \n",
    "                                                              fake_image_labels)\n",
    "            \n",
    "            # Add the discriminator accuracy and loss to our lists.\n",
    "            batch_discriminator_loss_real.append(d_loss_real)\n",
    "            batch_discriminator_loss_fake.append(d_loss_fake)\n",
    "            batch_discriminator_accuracy_real.append(d_accuracy_real)\n",
    "            batch_discriminator_accuracy_fake.append(d_accuracy_fake)\n",
    "            \n",
    "            ######################\n",
    "            # TRAIN THE GENERATOR.\n",
    "            ######################\n",
    "            \n",
    "            # Create the latent space. \n",
    "            latent_space = create_latent_space(latent_dimension,\n",
    "                                               batch_size)\n",
    "            \n",
    "            # Create array of labels... BUT... label as '1', despite the fact that they are fake. \n",
    "            fake_image_labels = np.ones(batch_size)\n",
    "            \n",
    "            # Feed it into the GAN.\n",
    "            generator_loss = gan_model.train_on_batch(latent_space, \n",
    "                                                      fake_image_labels)\n",
    "            \n",
    "            # Add the generator loss to our list.\n",
    "            batch_generator_loss.append(generator_loss)\n",
    "            \n",
    "            ###############\n",
    "            # LOG THE DATA.\n",
    "            ###############\n",
    "\n",
    "            # If we're at the end of the last bacth iteration.\n",
    "            if t == (num_batches-1):\n",
    "                \n",
    "                # Get the mean values of loss and accuracy. \n",
    "                epoch_discriminator_loss_real = sum(batch_discriminator_loss_real) / len(batch_discriminator_loss_real)\n",
    "                epoch_discriminator_loss_fake = sum(batch_discriminator_loss_fake) / len(batch_discriminator_loss_fake)\n",
    "                epoch_discriminator_accuracy_real = sum(batch_discriminator_accuracy_real) / len(batch_discriminator_accuracy_real)\n",
    "                epoch_discriminator_accuracy_fake = sum(batch_discriminator_accuracy_fake) / len(batch_discriminator_accuracy_fake)\n",
    "                epoch_generator_loss = sum(batch_generator_loss) / len(batch_generator_loss)\n",
    "        \n",
    "                # Horizontally concatenate them into a list. \n",
    "                epoch_data = [epoch_discriminator_loss_real, epoch_discriminator_loss_fake, epoch_discriminator_accuracy_real, epoch_discriminator_accuracy_fake, epoch_generator_loss]\n",
    "                \n",
    "                # Add the list to the pandas dataframe. \n",
    "                training_data.loc[len(training_data)] = epoch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os \n",
    "from keras.utils.vis_utils import plot_model\n",
    "import pydot \n",
    "import pydotplus\n",
    "import graphviz\n",
    "    \n",
    "# Function to train the GAN. \n",
    "def train_model(directory,\n",
    "               file_type = '.tif',\n",
    "               graph_model = True):\n",
    "    \n",
    "    ### (1) Establish variables useful for the rest of the code. \n",
    "    \n",
    "    now = datetime.now()\n",
    "    date_time = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    ### (2) Create and combine our models to make the GAN.\n",
    "    \n",
    "    # Create the discriminator. \n",
    "    img = cv2.imread(os.path.join(directory, 'image_1.tif'), -1)\n",
    "    img_height = img.shape[0]\n",
    "    img_width = img.shape[1]\n",
    "    img_channels = 1\n",
    "    discriminator_model = create_discriminator(img_height, \n",
    "                                               img_width, \n",
    "                                               img_channels)\n",
    "    \n",
    "    # Create the generator. \n",
    "    latent_size = 100 \n",
    "    generator_model = create_generator(latent_size,\n",
    "                                       img_height, \n",
    "                                       img_width)\n",
    "    \n",
    "    # Create the GAN.\n",
    "    gan_model = create_GAN(generator_model, discriminator_model)\n",
    "    \n",
    "    # Plot and save the model architechtures should the user desire it.\n",
    "    if graph_model == True: \n",
    "        \n",
    "        # If the folder to contain training graph etc. doesn't exist, create it. \n",
    "        folder_name = f'training data_{date_time}'\n",
    "        if not os.path.exists(os.path.join(directory, folder_name)):\n",
    "            os.makedirs(os.path.join(directory, folder_name))\n",
    "        \n",
    "        # Save the model architecture. \n",
    "        os.chdir(os.path.join(directory, folder_name))\n",
    "        \n",
    "        file_name = f'discriminator_flow_chart_{date_time}.png'\n",
    "        plot_model(discriminator_model, to_file=file_name, show_shapes=True, show_layer_names=True)\n",
    "\n",
    "        file_name = f'generator_flow_chart_{date_time}.png'\n",
    "        plot_model(generator_model, to_file=file_name, show_shapes=True, show_layer_names=True)\n",
    "        \n",
    "        file_name = f'GAN_flow_chart_{date_time}.png'\n",
    "        plot_model(gan_model, to_file=file_name, show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    # Create the series of for-loops which will train the generator and the discriminator.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dropout to the generator and the discriminator. \n",
    "\n",
    "# Figure out how the laent space (which is 2d now) can feed into the model. \n",
    "# I think it should only accept 1d shapes, might be wrong. \n",
    "\n",
    "# I need to make a function which selects a random batch of real images.\n",
    "# However, these images can't be accidentally shown again, or it might\n",
    "# bias the model. \n",
    "# solution: \n",
    "# 1 - Make range list. \n",
    "# Select from it, then delete those numbers. \n",
    "# Use those numbers for indexing. \n",
    "\n",
    "\n",
    "# Uninstall either atpbar or trange."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
